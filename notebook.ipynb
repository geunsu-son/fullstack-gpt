{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_chain(\"My name is Sony.\")\n",
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "file_list = os.listdir(\"./files\")\n",
    "for file_name in file_list:\n",
    "    loader = UnstructuredFileLoader(f\"./files/{file_name}\")\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)\n",
    "\n",
    "\n",
    "# stuff 방법으로 검색하기\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm\n",
    "\n",
    "print(chain.invoke(\"손근수라는 사람이 가진 기술과 강점을 정리해.\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_reduce 방법으로 검색하기\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim. If there is no relevant text, return : ''\n",
    "            -------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke(\n",
    "            {\"context\": doc.page_content, \"question\": question}\n",
    "        ).content\n",
    "        for doc in documents\n",
    "    )\n",
    "\n",
    "\n",
    "map_chain = {\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "\n",
    "print(chain.invoke(\"손근수라는 사람이 가진 기술과 강점을 정리해.\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Hugging Face Hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"[INST]What is the meaning of {word}[/INST]\")\n",
    "\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 250,\n",
    "    },\n",
    ")\n",
    "\n",
    "llm.client.api_url = (\n",
    "    \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"word\": \"potato\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# huggingface_pipeline : Using Model (Offline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"A {word} is a\")\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 150},\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"word\": \"tomato\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT4All : 작동 실패함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.gpt4all import GPT4All\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"You are a helpful assistant that defines words. Define this word: {word}.\"\n",
    ")\n",
    "\n",
    "llm = GPT4All(model=\"./falcon.bin\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"word\": \"tomato\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Function Calling\n",
    "\n",
    "질문에 답을 하는 대신에 함수를 사용하게 할 수 있음. (강제로 또는 선택적으로)  \n",
    "실제로 함수를 만드는 것이 아닌 함수의 설명을 제공함으로써 결과를 얻어냄.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What year was Rome founded?', 'answers': [{'answer': '753 BC', 'correct': True}, {'answer': '476 AD', 'correct': False}, {'answer': '1000 BC', 'correct': False}]}\n",
      "{'question': 'Who was the first emperor of Rome?', 'answers': [{'answer': 'Julius Caesar', 'correct': False}, {'answer': 'Augustus', 'correct': True}, {'answer': 'Nero', 'correct': False}]}\n",
      "{'question': 'What famous structure in Rome was built by the ancient Romans and is still standing today?', 'answers': [{'answer': 'Colosseum', 'correct': True}, {'answer': 'Pantheon', 'correct': False}, {'answer': 'Trevi Fountain', 'correct': False}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "function = {\n",
    "    \"name\": \"create_quiz\",\n",
    "    \"description\": \"function that takes a list of questions and answers and returns a quiz\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"questions\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"question\": {\n",
    "                            \"type\": \"string\",\n",
    "                        },\n",
    "                        \"answers\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"answer\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                    },\n",
    "                                    \"correct\": {\n",
    "                                        \"type\": \"boolean\",\n",
    "                                    },\n",
    "                                },\n",
    "                                \"required\": [\"answer\", \"correct\"],\n",
    "                            },\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"question\", \"answers\"],\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"questions\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ").bind(\n",
    "    function_call={\n",
    "        \"name\": \"create_quiz\",\n",
    "    },\n",
    "    functions=[\n",
    "        function,\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Make a quiz about {city}\")\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({\"city\": \"rome\"})\n",
    "\n",
    "\n",
    "response = response.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "\n",
    "import json\n",
    "\n",
    "for question in json.loads(response)[\"questions\"]:\n",
    "    print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Audio file & Using Whisper\n",
    "pydub : 오디오 파일을 List 형태처럼 촬용할 수 있는 라이브러리  \n",
    "glob : 원하는 파일 리스트를 얻을 수 있는 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pydub import AudioSegment\n",
    "import math\n",
    "import openai\n",
    "import glob\n",
    "\n",
    "\n",
    "def extract_audio_from_video(video_path, audio_path):\n",
    "    command = [\n",
    "        \"ffmpeg\",\n",
    "        \"-i\",\n",
    "        video_path,\n",
    "        \"-vn\",\n",
    "        audio_path,\n",
    "    ]\n",
    "    subprocess.run(command)\n",
    "\n",
    "\n",
    "def cut_audio_in_chunks(audio_path, chunk_size, chunks_folder):\n",
    "    track = AudioSegment.from_mp3(audio_path)\n",
    "    chunk_len = chunk_size * 60 * 1000\n",
    "    chunks = math.ceil(len(track) / chunk_len)\n",
    "    for i in range(chunks):\n",
    "        start_time = i * chunk_len\n",
    "        end_time = (i + 1) * chunk_len\n",
    "        chunk = track[start_time:end_time]\n",
    "        chunk.export(\n",
    "            f\"./{chunks_folder}/chunk_{i}.mp3\",\n",
    "            format=\"mp3\",\n",
    "        )\n",
    "\n",
    "\n",
    "def transcribe_chunks(chunk_folder, destination):\n",
    "    files = glob.glob(f\"{chunk_folder}/*.mp3\")\n",
    "    for file in files:\n",
    "        with open(file, \"rb\") as audio_file, open(destination, \"a\") as text_file:\n",
    "            transcript = openai.Audio.transcribe(\n",
    "                \"whisper-1\",\n",
    "                audio_file,\n",
    "            )\n",
    "            text_file.write(transcript[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import StructuredTool, Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "\n",
    "def plus(inputs):\n",
    "    a, b = inputs.split(\",\")\n",
    "    return float(a) + float(b)\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    tools=[\n",
    "        Tool.from_function(\n",
    "            func=plus,\n",
    "            name=\"Sum Calculator\",\n",
    "            description=\"Use this to perform sums of two numbers. Use this tool by sending a pair of number separated by a comma.\\nExample:1,2\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = \"Cost of $355.39 + $924.87 + $721.2 + $1940.29 + $573.63 + $65.72 + $35.00 + $552.00 + $76.16 + $29.12\"\n",
    "\n",
    "agent.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INVESTOR GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from typing import Type\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1, model_name=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "alpha_vantage_api_key = os.environ.get(\"ALPHA_VANTAGE_API_KEY\")\n",
    "\n",
    "\n",
    "class StockMarketSymbolSearchToolArgsSchema(BaseModel):\n",
    "    query: str = Field(\n",
    "        description=\"The query you will search for.Example query: Stock Market Symbol for Apple Company\"\n",
    "    )\n",
    "\n",
    "\n",
    "class StockMarketSymbolSearchTool(BaseTool):\n",
    "    name = \"StockMarketSymbolSearchTool\"\n",
    "    description = \"\"\"\n",
    "    Use this tool to find the stock market symbol for a company.\n",
    "    It takes a query as an argument.\n",
    "    \n",
    "    \"\"\"\n",
    "    args_schema: Type[\n",
    "        StockMarketSymbolSearchToolArgsSchema\n",
    "    ] = StockMarketSymbolSearchToolArgsSchema\n",
    "\n",
    "    def _run(self, query):\n",
    "        ddg = DuckDuckGoSearchAPIWrapper()\n",
    "        return ddg.run(query)\n",
    "\n",
    "\n",
    "class CompanyOverviewArgsSchema(BaseModel):\n",
    "    symbol: str = Field(\n",
    "        description=\"Stock symbol of the company.Example: AAPL,TSLA\",\n",
    "    )\n",
    "\n",
    "\n",
    "class CompanyOverviewTool(BaseTool):\n",
    "    name = \"CompanyOverview\"\n",
    "    description = \"\"\"\n",
    "    Use this to get an overview of the financials of the company.\n",
    "    You should enter a stock symbol.\n",
    "    \"\"\"\n",
    "    args_schema: Type[CompanyOverviewArgsSchema] = CompanyOverviewArgsSchema\n",
    "\n",
    "    def _run(self, symbol):\n",
    "        r = requests.get(\n",
    "            f\"https://www.alphavantage.co/query?function=OVERVIEW&symbol={symbol}&apikey={alpha_vantage_api_key}\"\n",
    "        )\n",
    "        return r.json()\n",
    "\n",
    "\n",
    "class CompanyIncomeStatementTool(BaseTool):\n",
    "    name = \"CompanyIncomeStatement\"\n",
    "    description = \"\"\"\n",
    "    Use this to get the income statement of a company.\n",
    "    You should enter a stock symbol.\n",
    "    \"\"\"\n",
    "    args_schema: Type[CompanyOverviewArgsSchema] = CompanyOverviewArgsSchema\n",
    "\n",
    "    def _run(self, symbol):\n",
    "        r = requests.get(\n",
    "            f\"https://www.alphavantage.co/query?function=INCOME_STATEMENT&symbol={symbol}&apikey={alpha_vantage_api_key}\"\n",
    "        )\n",
    "        return r.json()[\"annualReports\"]\n",
    "\n",
    "\n",
    "class CompanyStockPerformanceTool(BaseTool):\n",
    "    name = \"CompanyStockPerformance\"\n",
    "    description = \"\"\"\n",
    "    Use this to get the weekly performance of a company stock.\n",
    "    You should enter a stock symbol.\n",
    "    \"\"\"\n",
    "    args_schema: Type[CompanyOverviewArgsSchema] = CompanyOverviewArgsSchema\n",
    "\n",
    "    def _run(self, symbol):\n",
    "        r = requests.get(\n",
    "            f\"https://www.alphavantage.co/query?function=TIME_SERIES_WEEKLY&symbol={symbol}&apikey={alpha_vantage_api_key}\"\n",
    "        )\n",
    "        return r.json()\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    handle_parsing_errors=True,\n",
    "    tools=[\n",
    "        StockMarketSymbolSearchTool(),\n",
    "        CompanyOverviewTool(),\n",
    "        CompanyIncomeStatementTool(),\n",
    "        CompanyStockPerformanceTool(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = \"Give me financial information on Cloudflare's stock, considering its financials, income statements and stock performance help me analyze if it's a potential good investment.\"\n",
    "\n",
    "agent.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLDatabaseToolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_sql_agent, AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    model_name=\"gpt-4-1106-preview\",\n",
    ")\n",
    "db = SQLDatabase.from_uri(\"sqlite:///movies.sqlite\")\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "\n",
    "agent = create_sql_agent(\n",
    "    llm=llm,\n",
    "    toolkit=toolkit,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent.invoke(\n",
    "    \"Give me the movies that have the highest votes but the lowest budgets and give me the name of their directors also include their gross revenue.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone\n",
    "무료 플랜을 이용한 벡터 스토어 활용해보기  \n",
    "* [Pinecone 사이트](https://www.pinecone.io/)에서 index를 만들고 진행할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "import os\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
    "    environment=\"gcp-starter\",\n",
    ")\n",
    "\n",
    "# 1번 실행하여 벡터 스토어에 적용한 후에는 다시 실행할 필요 X\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder()\n",
    "loader = CSVLoader(\"../recipes.csv\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "vector_store = Pinecone.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    index_name=\"recipes\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 스토어에 기록한 데이터 불러오기\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = Pinecone.from_existing_index(\n",
    "    \"recipes\",\n",
    "    embeddings,\n",
    ")\n",
    "\n",
    "docs = vector_store.similarity_search(\"tofu\")\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSISTANT API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용할 함수 선언 : OpenAI 사이트에서 추가된 내용 확인 가능\n",
    "OpenAI playground에서 실사용해볼 수 있다.  \n",
    "특이한 점은 함수를 실행했을 때의 output을 직접 입력하는 과정이 있었다는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "\n",
    "def get_ticker(inputs):\n",
    "    ddg = DuckDuckGoSearchAPIWrapper()\n",
    "    company_name = inputs[\"company_name\"]\n",
    "    return ddg.run(f\"Ticker symbol of {company_name}\")\n",
    "\n",
    "\n",
    "functions = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_ticker\",\n",
    "            \"description\": \"Given the name of a company returns its ticker symbol\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"company_name\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name of the company\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"company_name\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_income_statement\",\n",
    "            \"description\": \"Given a ticker symbol (i.e AAPL) returns the company's income statement.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"ticker\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Ticker symbol of the company\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"ticker\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_balance_sheet\",\n",
    "            \"description\": \"Given a ticker symbol (i.e AAPL) returns the company's balance sheet.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"ticker\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Ticker symbol of the company\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"ticker\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_daily_stock_performance\",\n",
    "            \"description\": \"Given a ticker symbol (i.e AAPL) returns the performance of the stock for the last 100 days.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"ticker\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Ticker symbol of the company\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"ticker\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai as client\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Investor Assistant\",\n",
    "    instructions=\"You help users do research on publicly traded companies and you help users decide if they should buy the stock or not.\",\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    tools=functions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thread ~ Run Python 환경에서 해보기\n",
    "진행 상황을 Streaming하는 기능이 없을 때를 기준으로 강의는 제작되었음.  \n",
    "현재는 Streaming 기능을 제공하는 것으로 보여짐. ([출처](https://platform.openai.com/docs/api-reference/assistants-streaming-v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\n",
    "import yfinance\n",
    "import json\n",
    "\n",
    "\n",
    "def get_ticker(inputs):\n",
    "    ddg = DuckDuckGoSearchAPIWrapper()\n",
    "    company_name = inputs[\"company_name\"]\n",
    "    return ddg.run(f\"Ticker symbol of {company_name}\")\n",
    "\n",
    "\n",
    "def get_income_statement(inputs):\n",
    "    ticker = inputs[\"ticker\"]\n",
    "    stock = yfinance.Ticker(ticker)\n",
    "    return json.dumps(stock.income_stmt.to_json())\n",
    "\n",
    "\n",
    "def get_balance_sheet(inputs):\n",
    "    ticker = inputs[\"ticker\"]\n",
    "    stock = yfinance.Ticker(ticker)\n",
    "    return json.dumps(stock.balance_sheet.to_json())\n",
    "\n",
    "\n",
    "def get_daily_stock_performance(inputs):\n",
    "    ticker = inputs[\"ticker\"]\n",
    "    stock = yfinance.Ticker(ticker)\n",
    "    return json.dumps(stock.history(period=\"3mo\").to_json())\n",
    "\n",
    "\n",
    "functions_map = {\n",
    "    \"get_ticker\": get_ticker,\n",
    "    \"get_income_statement\": get_income_statement,\n",
    "    \"get_balance_sheet\": get_balance_sheet,\n",
    "    \"get_daily_stock_performance\": get_daily_stock_performance,\n",
    "}\n",
    "\n",
    "\n",
    "functions = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_ticker\",\n",
    "            \"description\": \"Given the name of a company returns its ticker symbol\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"company_name\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name of the company\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"company_name\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_income_statement\",\n",
    "            \"description\": \"Given a ticker symbol (i.e AAPL) returns the company's income statement.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"ticker\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Ticker symbol of the company\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"ticker\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_balance_sheet\",\n",
    "            \"description\": \"Given a ticker symbol (i.e AAPL) returns the company's balance sheet.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"ticker\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Ticker symbol of the company\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"ticker\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_daily_stock_performance\",\n",
    "            \"description\": \"Given a ticker symbol (i.e AAPL) returns the performance of the stock for the last 100 days.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"ticker\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Ticker symbol of the company\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"ticker\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai as client\n",
    "\n",
    "# assistant = client.beta.assistants.create(\n",
    "#     name=\"Investor Assistant\",\n",
    "#     instructions=\"You help users do research on publicly traded companies and you help users decide if they should buy the stock or not.\",\n",
    "#     model=\"gpt-4-1106-preview\",\n",
    "#     tools=functions,\n",
    "# )\n",
    "\n",
    "assistant_id = \"asst_RJHxXodfpEkpDGua4NL2wXAC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread의 경우 다시 선언하면 기존에 했던 대화가 기록되지 않은 새로운 대화가 생성되기 때문에 1번만 실행\n",
    "thread = client.beta.threads.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I want to know if the Salesforce stock is a good buy\",\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run(run_id, thread_id):\n",
    "    return client.beta.threads.runs.retrieve(\n",
    "        run_id=run_id,\n",
    "        thread_id=thread_id,\n",
    "    )\n",
    "\n",
    "\n",
    "def send_message(thread_id, content):\n",
    "    return client.beta.threads.messages.create(\n",
    "        thread_id=thread_id, role=\"user\", content=content\n",
    "    )\n",
    "\n",
    "\n",
    "def get_messages(thread_id):\n",
    "    messages = client.beta.threads.messages.list(thread_id=thread_id)\n",
    "    messages = list(messages)\n",
    "    messages.reverse()\n",
    "    for message in messages:\n",
    "        print(f\"{message.role}: {message.content[0].text.value}\")\n",
    "\n",
    "\n",
    "def get_tool_outputs(run_id, thread_id):\n",
    "    run = get_run(run_id, thread.id)\n",
    "    outputs = []\n",
    "    for action in run.required_action.submit_tool_outputs.tool_calls:\n",
    "        action_id = action.id\n",
    "        function = action.function\n",
    "        print(f\"Calling function: {function.name} with arg {function.arguments}\")\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"output\": functions_map[function.name](json.loads(function.arguments)),\n",
    "                \"tool_call_id\": action_id,\n",
    "            }\n",
    "        )\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def submit_tool_outputs(run_id, thread_id):\n",
    "    outputs = get_tool_outputs(run_id, thread_id)\n",
    "    return client.beta.threads.runs.submit_tool_outputs(\n",
    "        run_id=run_id, thread_id=thread_id, tool_outputs=outputs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submit_tool_outputs(run.id, thread.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run의 현재 상황을 확인해보기\n",
    "get_run(run.id, thread.id).status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thread에서 진행된 대화 확인하기\n",
    "get_messages(thread.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
