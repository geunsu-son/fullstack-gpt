from langchain.document_loaders import SitemapLoader
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
import streamlit as st
import requests
import xml.etree.ElementTree as ET
from urllib.parse import urlparse
from langchain.document_loaders import WebBaseLoader
import os
from datetime import datetime

st.set_page_config(
    page_title="SiteGPT",
    page_icon="ğŸ–¥ï¸",
)

st.title("Cloudflare AI Assistant")
st.markdown("""
ì´ AI ì–´ì‹œìŠ¤í„´íŠ¸ëŠ” Cloudflareì˜ AI ì œí’ˆë“¤ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- AI Gateway
- Vectorize
- Workers AI

ë¬¸ì„œ ì¶œì²˜: [Cloudflare Developers](https://developers.cloudflare.com/)
""")

with st.sidebar:
    st.markdown("[ğŸ”— Git Repo Link](https://github.com/geunsu-son/fullstack-gpt)")
    openai_api_key = st.text_input("Enter your OpenAI API key", type="password")
    if st.button("ë²¡í„° ì €ì¥ì†Œ ìƒˆë¡œê³ ì¹¨"):
        if os.path.exists("./.cache/vector_store"):
            import shutil
            shutil.rmtree("./.cache/vector_store")
        st.cache_data.clear()
        st.success("ë²¡í„° ì €ì¥ì†Œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ë©´ ë¬¸ì„œë¥¼ ë‹¤ì‹œ ë¡œë“œí•©ë‹ˆë‹¤.")

if not openai_api_key:
    st.warning("Please enter your OpenAI API key in the sidebar")
    st.stop()

# Initialize LLM with the API key from sidebar
llm = ChatOpenAI(
    temperature=0.1,
    api_key=openai_api_key,
)

answers_prompt = ChatPromptTemplate.from_template(
    """
    You are an AI assistant specialized in Cloudflare's AI products documentation. Using ONLY the following context, answer the user's question.
    If you can't find the answer in the context, just say "I don't know" - do not make up information.
    
    Always mention which Cloudflare product (AI Gateway, Vectorize, or Workers AI) you're referring to in your answer.
    Include relevant pricing, limits, or technical specifications when available.
    
    Context: {context}
    
    Question: {question}
    
    Score (0-5): Rate how well the context helps answer the question.
    """
)


def get_answers(inputs):
    docs = inputs["docs"]
    question = inputs["question"]
    answers_chain = answers_prompt | llm
    return {
        "question": question,
        "answers": [
            {
                "answer": answers_chain.invoke(
                    {"question": question, "context": doc.page_content}
                ).content,
                "source": doc.metadata.get("source", "Unknown"),
                # lastmodê°€ ì—†ì„ ê²½ìš° í˜„ì¬ ë‚ ì§œ ì‚¬ìš©
                "date": doc.metadata.get("lastmod", datetime.now().strftime("%Y-%m-%d")),
                "product": doc.metadata.get("product", "Unknown")
            }
            for doc in docs
        ],
    }


choose_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            You are a Cloudflare AI products documentation expert. Use ONLY the following pre-existing answers to respond to the user's question.
            
            Focus on answers with the highest score (most relevant) and most recent dates.
            Always include the specific product name (AI Gateway, Vectorize, or Workers AI) in your response.
            
            When available, include:
            - Pricing information
            - Usage limits
            - Technical specifications
            - Implementation details
            
            Cite sources using their URLs and include the last modification dates for transparency.
            Format URLs as markdown links: [Product Name](URL)
            
            Answers: {answers}
            """,
        ),
        ("human", "{question}"),
    ]
)


def choose_answer(inputs):
    answers = inputs["answers"]
    question = inputs["question"]
    choose_chain = choose_prompt | llm
    
    # Sort answers by score and date
    condensed = "\n\n".join(
        f"Product: {answer.get('product', 'Unknown')}\n{answer['answer']}\nSource: [{answer['source']}]({answer['source']})\nLast modified: {answer['date']}\n"
        for answer in answers
    )
    
    return choose_chain.invoke(
        {
            "question": question,
            "answers": condensed,
        }
    )


def parse_page(soup):
    header = soup.find("header")
    footer = soup.find("footer")
    if header:
        header.decompose()
    if footer:
        footer.decompose()
    return (
        str(soup.get_text())
        .replace("\n", " ")
        .replace("\xa0", " ")
        .replace("CloseSearch Submit Blog", "")
    )

@st.cache_data(show_spinner="Loading Cloudflare documentation...")
def load_cloudflare_docs():
    try:
        # ì €ì¥ëœ ë²¡í„° ì €ì¥ì†Œ í™•ì¸
        vector_store_dir = "./.cache/vector_store"
        os.makedirs(vector_store_dir, exist_ok=True)
        vector_store_path = os.path.join(vector_store_dir, "cloudflare_docs_store")
        
        if os.path.exists(vector_store_path):
            st.write("ì €ì¥ëœ ë²¡í„° ì €ì¥ì†Œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
            vector_store = FAISS.load_local(vector_store_path, OpenAIEmbeddings(openai_api_key=openai_api_key))
            return vector_store.as_retriever(search_kwargs={"k": 4})
        
        # 1. sitemapì—ì„œ URL ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        sitemap_url = 'https://developers.cloudflare.com/sitemap-0.xml'
        
        response = requests.get(sitemap_url)
        root = ET.fromstring(response.content)
        namespaces = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
        
        # URLê³¼ lastmod ì •ë³´ í•¨ê»˜ ê°€ì ¸ì˜¤ê¸°
        urls_with_dates = []
        for url in root.findall('.//ns:loc', namespaces):
            lastmod = url.find('../ns:lastmod', namespaces)
            lastmod_date = lastmod.text if lastmod is not None else datetime.now().strftime("%Y-%m-%d")
            urls_with_dates.append((url.text, lastmod_date))
        
        # 2. URL í•„í„°ë§
        filter_patterns = ["ai-gateway", "vectorize", "workers-ai"]
        filtered_urls = [(url, date) for url, date in urls_with_dates 
                        if any(pattern in url for pattern in filter_patterns)]
        
        # 3. í•„í„°ë§ëœ URLì—ì„œ ë¬¸ì„œ ìˆ˜ì§‘
        all_docs = []
        status_container = st.empty()
        
        for idx, (url, lastmod_date) in enumerate(filtered_urls):
            status_container.text(f"ë¬¸ì„œ ë¡œë”© ì¤‘... ({idx + 1}/{len(filtered_urls)})")
            try:
                loader = WebBaseLoader(
                    url,
                    default_parser="lxml",
                    bs_kwargs={"parser": "lxml", "features": "lxml"}
                )
                docs = loader.load()
                
                # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                for doc in docs:
                    doc.metadata["lastmod"] = lastmod_date
                    if "ai-gateway" in url:
                        doc.metadata["product"] = "AI Gateway"
                    elif "vectorize" in url:
                        doc.metadata["product"] = "Cloudflare Vectorize"
                    elif "workers-ai" in url:
                        doc.metadata["product"] = "Workers AI"
                all_docs.extend(docs)
            except Exception as e:
                continue
        
        status_container.empty()
        
        if not all_docs:
            st.error("ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None
        
        # 4. ë¬¸ì„œ ë¶„í• 
        splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            chunk_size=1000,
            chunk_overlap=200,
        )
        split_docs = splitter.split_documents(all_docs)
        
        # 5. ë²¡í„° ì €ì¥ì†Œ ìƒì„± ë° ì €ì¥
        embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
        vector_store = FAISS.from_documents(split_docs, embeddings)
        vector_store.save_local(vector_store_path)
        
        return vector_store.as_retriever(search_kwargs={"k": 4})
        
    except Exception as e:
        st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return None

# Main interface
retriever = load_cloudflare_docs()

if retriever is None:
    st.stop()

# ì˜ˆì‹œ ì§ˆë¬¸ ì„¹ì…˜
st.divider()
st.markdown("### ì˜ˆì‹œ ì§ˆë¬¸")
example_questions = [
    "llama-2-7b-chat-fp16 ëª¨ë¸ì˜ 1M ì…ë ¥ í† í°ë‹¹ ê°€ê²©ì€ ì–¼ë§ˆì¸ê°€ìš”?",
    "Cloudflareì˜ AI ê²Œì´íŠ¸ì›¨ì´ë¡œ ë¬´ì—‡ì„ í•  ìˆ˜ ìˆë‚˜ìš”?",
    "ë²¡í„°ë¼ì´ì¦ˆì—ì„œ ë‹¨ì¼ ê³„ì •ì€ ëª‡ ê°œì˜ ì¸ë±ìŠ¤ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‚˜ìš”?"
]

# ë²„íŠ¼ ì»¬ëŸ¼ ìƒì„±
cols = st.columns(len(example_questions))
clicked_question = None

# ê° ì˜ˆì‹œ ì§ˆë¬¸ì„ ë²„íŠ¼ìœ¼ë¡œ ë§Œë“¤ê¸°
for idx, question in enumerate(example_questions):
    col1, col2 = st.columns([0.8, 0.2])
    with col1:
        st.write(f"* {question}")
    with col2:
        if st.button(f"ì˜ˆì‹œ {idx + 1}", key=f"example_{idx}"):
            clicked_question = question

st.divider()
st.markdown("### Cloudflare AI ì œí’ˆì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”")
# ì‚¬ìš©ì ì…ë ¥ í•„ë“œ (ì˜ˆì‹œ ì§ˆë¬¸ì´ ì„ íƒë˜ë©´ ê·¸ ì§ˆë¬¸ìœ¼ë¡œ ì±„ì›Œì§)
query = st.text_input(
    "",
    value=clicked_question if clicked_question else "",
    key="user_question"
)

if query:
    with st.spinner("ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        chain = (
            {
                "docs": retriever,
                "question": RunnablePassthrough(),
            }
            | RunnableLambda(get_answers)
            | RunnableLambda(choose_answer)
        )
        result = chain.invoke(query)
        st.markdown(result.content.replace("$", "\\$"))
